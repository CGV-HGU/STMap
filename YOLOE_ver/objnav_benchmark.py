import habitat
import os
import argparse
import csv
import cv2
import imageio
import numpy as np
from tqdm import tqdm
from constants import *
from config_utils import hm3d_config
from gpt4v_planner import GPT4V_Planner
from policy_agent import Policy_Agent
from habitat.utils.visualizations.maps import colorize_draw_agent_and_fit_to_height

# ğŸ” YOLOE ì „ìš© ìœ í‹¸ (ì„¸ê·¸ ì „ìš©)
from cv_utils.yoloe_tools import (
    initialize_yoloe_model,
    yoloe_detection,
)

os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ["MAGNUM_LOG"] = "quiet"
os.environ["HABITAT_SIM_LOG"] = "quiet"

def write_metrics(metrics, path="objnav_hm3d.csv"):
    with open(path, mode="w", newline="") as csv_file:
        fieldnames = metrics[0].keys()
        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(metrics)

def adjust_topdown(metrics):
    return cv2.cvtColor(colorize_draw_agent_and_fit_to_height(metrics['top_down_map'], 1024), cv2.COLOR_BGR2RGB)

def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--eval_episodes", type=int, default=200)
    return parser.parse_known_args()[0]

# âœ… bbox ì—†ì´ ì„¸ê·¸ë¨¼íŠ¸ë§Œ ì“°ëŠ” ê°ì§€ í•¨ìˆ˜
def detect_mask(image, category, yoloe_model):
    """
    YOLOE ì„¸ê·¸ë§Œ ì‚¬ìš©í•´ category ë§ˆìŠ¤í¬ë¥¼ ì–»ëŠ”ë‹¤.
    - ì„±ê³µ ì‹œ: (True, image, mask(H,W){0,1})
    - ì‹¤íŒ¨ ì‹œ: (False, [], [])
    """
    det = yoloe_detection(
        image=image,
        target_classes=[category],   # í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ 1ê°œ
        model=yoloe_model,
        box_threshold=0.25,
        iou_threshold=0.50,
        run_extra_nms=False,
        use_text_prompt=True,
        retina_masks=True,
    )
    if det.masks is None or det.class_id.size == 0:
        return False, [], []

    # í”„ë¡¬í”„íŠ¸ê°€ [category] í•˜ë‚˜ì´ë¯€ë¡œ class_id==0 ì¸ ê²ƒë“¤ ì¤‘ 'ë©´ì  ìµœëŒ€' ì„ íƒ
    idxs = np.where(det.class_id == 0)[0]
    if len(idxs) == 0:
        return False, [], []
    areas = [det.masks[i].sum() for i in idxs]
    top = int(idxs[int(np.argmax(areas))])
    mask = det.masks[top].astype(np.uint8)
    return True, image, mask


args = get_args()
habitat_config = hm3d_config(stage='val', episodes=args.eval_episodes)
print("scene_dataset =", habitat_config.habitat.simulator.scene_dataset)
print("scenes_dir    =", habitat_config.habitat.dataset.scenes_dir)
print("data_path     =", habitat_config.habitat.dataset.data_path)
habitat_env = habitat.Env(habitat_config)

# âœ… YOLOE ì´ˆê¸°í™” (ì„¸ê·¸ ê°€ì¤‘ì¹˜ í•„ìˆ˜)
#    detect_objectsëŠ” í”Œë˜ë„ˆì—ì„œ ì“°ëŠ” í´ë˜ìŠ¤ ë¦¬ìŠ¤íŠ¸ì™€ ì¼ì¹˜ì‹œì¼œ ë‘ë©´ ì¢‹ì•„.
DETECT_OBJECTS = ['bed', 'sofa', 'chair', 'plant', 'tv', 'toilet', 'floor']
yoloe_model = initialize_yoloe_model(
    weights=YOLOE_CHECKPOINT_PATH,   # ì„¸ê·¸ ì§€ì› ê°€ì¤‘ì¹˜
    device="cuda:0",
    classes=DETECT_OBJECTS,       # í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ê¸°ë³¸ ì„¸íŒ…
    prompt_mode="text",
    conf_threshold=0.25,
    iou_threshold=0.50,
)

# âœ… í”Œë˜ë„ˆ/ì—ì´ì „íŠ¸
#   - í”Œë˜ë„ˆê°€ YOLOE í•œ ê°œë§Œ ë°›ë„ë¡ ë°”ê¿¨ë‹¤ë©´ â†“ ê·¸ëŒ€ë¡œ ì‚¬ìš©
#   - ì—¬ì „íˆ (dino_model, sam_model) 2ê°œ ì¸ìë¥¼ ë°›ëŠ” ì˜› ë²„ì „ì´ë¼ë©´ ê°™ì€ ëª¨ë¸ì„ ë‘ ë²ˆ ë„£ëŠ”ë‹¤.
try:
    nav_planner = GPT4V_Planner(yoloe_model)
except TypeError:
    # fallback: ì˜› ì‹œê·¸ë‹ˆì²˜ í˜¸í™˜
    nav_planner = GPT4V_Planner(yoloe_model, yoloe_model)

nav_executor = Policy_Agent(model_path=POLICY_CHECKPOINT)
evaluation_metrics = []

for i in tqdm(range(args.eval_episodes)):
    find_goal = False
    obs = habitat_env.reset()
    dir = "./tmp/trajectory_%d" % i
    os.makedirs(dir, exist_ok=False)
    fps_writer = imageio.get_writer("%s/fps.mp4" % dir, fps=4)
    topdown_writer = imageio.get_writer("%s/metric.mp4" % dir, fps=4)
    heading_offset = 0
    start_geodesic_m = float(habitat_env.get_metrics()['distance_to_goal'])  

    nav_planner.reset(habitat_env.current_episode.object_category)
    episode_images = [obs['rgb']]
    episode_topdowns = [adjust_topdown(habitat_env.get_metrics())]

    # a whole round planning process
    for _ in range(11):
        obs = habitat_env.step(3)
        episode_images.append(obs['rgb'])
        episode_topdowns.append(adjust_topdown(habitat_env.get_metrics()))
    goal_image, goal_mask, debug_image, goal_rotate, goal_flag, _ = nav_planner.make_plan(episode_images[-12:])
    for j in range(min(11 - goal_rotate, 1 + goal_rotate)):
        if goal_rotate <= 6:
            obs = habitat_env.step(3)
            episode_images.append(obs['rgb'])
            episode_topdowns.append(adjust_topdown(habitat_env.get_metrics()))
        else:
            obs = habitat_env.step(2)
            episode_images.append(obs['rgb'])
            episode_topdowns.append(adjust_topdown(habitat_env.get_metrics()))
    nav_executor.reset(goal_image, goal_mask)

    while not habitat_env.episode_over:
        action, skill_image = nav_executor.step(obs['rgb'], habitat_env.sim.previous_step_collided)
        if action != 0 or goal_flag:
            if action == 4:
                heading_offset += 1
            elif action == 5:
                heading_offset -= 1
            obs = habitat_env.step(action)
            episode_images.append(obs['rgb'])
            episode_topdowns.append(adjust_topdown(habitat_env.get_metrics()))
        else:
            if habitat_env.episode_over:
                break

            for _ in range(0, abs(heading_offset)):
                if habitat_env.episode_over:
                    break
                if heading_offset > 0:
                    obs = habitat_env.step(5)
                    episode_images.append(obs['rgb'])
                    episode_topdowns.append(adjust_topdown(habitat_env.get_metrics()))
                    heading_offset -= 1
                elif heading_offset < 0:
                    obs = habitat_env.step(4)
                    episode_images.append(obs['rgb'])
                    episode_topdowns.append(adjust_topdown(habitat_env.get_metrics()))
                    heading_offset += 1

            # a whole round planning process
            for _ in range(11):
                if habitat_env.episode_over:
                    break
                obs = habitat_env.step(3)
                episode_images.append(obs['rgb'])
                episode_topdowns.append(adjust_topdown(habitat_env.get_metrics()))
            goal_image, goal_mask, debug_image, goal_rotate, goal_flag, _ = nav_planner.make_plan(episode_images[-12:])
            for j in range(min(11 - goal_rotate, goal_rotate + 1)):
                if habitat_env.episode_over:
                    break
                if goal_rotate <= 6:
                    obs = habitat_env.step(3)
                    episode_images.append(obs['rgb'])
                    episode_topdowns.append(adjust_topdown(habitat_env.get_metrics()))
                else:
                    obs = habitat_env.step(2)
                    episode_images.append(obs['rgb'])
                    episode_topdowns.append(adjust_topdown(habitat_env.get_metrics()))
            nav_executor.reset(goal_image, goal_mask)

    for image, topdown in zip(episode_images, episode_topdowns):
        fps_writer.append_data(image)
        topdown_writer.append_data(topdown)
    fps_writer.close()
    topdown_writer.close()

    evaluation_metrics.append({
        'episode': i,
        'object_goal': habitat_env.current_episode.object_category,
        'success': habitat_env.get_metrics()['success'],
        'spl': habitat_env.get_metrics()['spl'],
        'start_distance_to_goal': start_geodesic_m, 
        'final_distance_to_goal': habitat_env.get_metrics()['distance_to_goal'],
        'llm_calls': int(nav_planner.llm_call_count),
        'llm_avg_time_sec': float(np.mean(nav_planner.llm_durations)) if len(nav_planner.llm_durations) > 0 else 0.0,
        'planner_avg_time_sec': float(np.mean(nav_planner.planner_durations)) if len(nav_planner.planner_durations) > 0 else 0.0,
    })

    write_metrics(evaluation_metrics)
